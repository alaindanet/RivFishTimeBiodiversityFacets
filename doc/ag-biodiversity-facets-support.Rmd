---
title: "Biodiversity facets supporting material"
author: "Alain Danet"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  bookdown::html_document2:
    df_print: paged
    toc: true
    toc_float: true
    theme: "readable"
    code_folding: hide
bibliography: "bibliography.bib"
---

```{r setup, include=FALSE}
source(here::here("start_rmd.R"))
#knitr::opts_chunk$set(cache = TRUE)
library(skimr)
library(dotwhisker)
library(broom.mixed)
library(ade4)
library(factoextra)
```

```{r load-targets, include=FALSE}
tar_load(c(modelling_data, analysis_dataset))
tar_load(c(abun_var, var_jaccard, rich_var, clust_var, main_effect_var))
tar_load(site_cl_rm)
```

# Data exploration

```{r}
library(gt)
library(gtsummary)
```

- Response variable

```{r}
get_tbl_summary <- function(df = NULL) {

t1 <- df %>%
  tbl_summary(statistic = all_continuous() ~ "{N_nonmiss}", missing = "no") %>%
  modify_header(stat_0 ~ "**N (no missing)**")

t2 <- df %>%
  tbl_summary(statistic = all_continuous() ~
    "{median} ({p25}, {p75})",
  missing = "no") %>%
  modify_header(stat_0 ~ "Median (1st Q, 3rd Q)")

t3 <- df %>%
  tbl_summary(statistic = all_continuous() ~
    "({min}, {max})",
  missing = "no") %>%
  modify_header(stat_0 ~ "(Min, Max)")

tbl_merge(list(t1, t2, t3)) %>%
  modify_footnote(everything() ~ NA_character_) %>%
  modify_spanning_header(everything() ~ NA_character_)

}

resp_summary <- get_tbl_summary(df = modelling_data %>%
  select(all_of(c("chao_richness", "species_richness",
        clust_var, "year", "year_nb"))))

resp_summary
```

- Environmental variables:

```{r}
col_pca_riv <- c("siteid", "dist_up_km", "ord_stra", "dis_m3_pyr", "ele_mt_cav", "slp_dg_cav")
tar_load(riveratlas_site)
riveratlas_site <- riveratlas_site %>%
  select(all_of(col_pca_riv)) %>%
  st_drop_geometry() %>%
  left_join(
    modelling_data %>%
      distinct(siteid, hft_ix_c93, hft_ix_c9309_log2_ratio),
    by = "siteid") %>%
  select(-siteid) %>%
  rename_with(~str_replace_all(.,
      c(
        get_model_term_replacement(),
        get_rev_vec_name_val(get_river_atlas_significant_var())
      )
    )
  )

evt_summary <- get_tbl_summary(df = riveratlas_site)
evt_summary
```

- Protocol:

```{r}
tar_load(filtered_dataset)
protocol <- filtered_dataset$site_quali %>%
  filter(siteid %in% unique(modelling_data$siteid)) %>%
  select(protocol_detail, unitabundance)

protocol_summary <- get_tbl_summary(df = protocol)
protocol_summary
```

- Location: 

```{r}
location_site <- filtered_dataset$location %>%
  filter(siteid %in% unique(modelling_data$siteid)) %>%
  select(country, ecoregion)

location_summary <- location_site %>%
  tbl_summary()
location_summary
```


# Visualise interactions 

```{r}
jaccard <- gaussian_int_env$mod[[1]]
nestedness <- gaussian_int_env$mod[[3]]
slopes <- estimate_slopes(nestedness, trend = "log1_year_nb", at = c("riv_str_rc1",
    "hft_ix_c93"))
plot(slopes)
```

# Heat map

```{r}
```

# Comparison drivers / no drivers model on random effects 

- We compared the estimated temporal trends of sites in a model without drivers
  and with drivers in order to see the effect of the drivers.

We see that the estimations are very well correlated, while the model without
drivers underestimate sometimes the estimated temporal trends.

```{r}
tar_load(p_comp_site_tps_trends_drivers_tmb)
p_comp_site_tps_trends_drivers_tmb
```



# Notes on transformed variables 

- Notes:
  - [Back transform coefficients on scaled predictors](https://stackoverflow.com/a/47007597)
  - [Back transform coefficients on scaled predictors](https://stackoverflow.com/a/24286763)
  - [Back transform coefficients on scaled predictors](https://stackoverflow.com/a/53735644)
  - 

- [Interpreting coefficients on log scale](https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/):
  - Only the dependent/response variable is log-transformed: Exponentiate the
    coefficient, subtract one from this number, and multiply by 100. This gives
    the percent increase (or decrease) in the response for every one-unit
    increase in the independent variable. Example: the coefficient is 0.198.
    (exp(0.198) – 1) * 100 = 21.9. For every one-unit increase in the
    independent variable, our dependent variable increases by about 22%.
 - Only independent/predictor variable(s) is log-transformed: Divide the
   coefficient by 100. This tells us that a 1% increase in the independent
   variable increases (or decreases) the dependent variable by (coefficient/100)
   units. Example: the coefficient is 0.198. 0.198/100 = 0.00198. For every 1%
   increase in the independent variable, our dependent variable increases by
   about 0.002. For x percent increase, multiply the coefficient by log(1.x).
   Example: For every 10% increase in the independent variable, our dependent
   variable increases by about 0.198 * log(1.10) = 0.02.
  - Both dependent/response variable and independent/predictor variable(s) are
    log-transformed: Interpret the coefficient as the percent increase in the
    dependent variable for every 1% increase in the independent variable.
    Example: the coefficient is 0.198. For every 1% increase in the independent
    variable, our dependent variable increases by about 0.20%. For x percent
    increase, calculate 1.x to the power of the coefficient, subtract 1, and
    multiply by 100. Example: For every 20% increase in the independent
    variable, our dependent variable increases by about $(1.20^0.198 – 1) * 100$ =
    3.7 percent.

# Environmental variables 

```{r}
cor_hft <- cor(analysis_dataset[,
    c("hft_c9309_scaled_no_center", "hft_ix_c9309_log2_ratio", "hft_ix_c93", "hft_ix_c09",
      "log_dist_up_km")
    ] %>%
      na.omit(),
  method = "spearman"
)
dimnames(cor_hft) <- map(
  dimnames(cor_hft),
  ~get_hft_riv_var()[.x]
)
ggcorrplot::ggcorrplot(
  cor_hft, hc.order = TRUE, type = "lower",
  lab = TRUE, tl.srt = 15, legend.title = "Spearman Corr") +
theme(legend.position = c(.45, .95), legend.direction = "horizontal") +
guides(colour = guide_legend(title.position = "top"))
```
# Heatmap cluster and multinomial model  

```{r cl-heat}
te <- expand.grid(
  list(
    cluster = c(seq_len(6)),
    response = clust_var
  )
)

avg_cl_var <- site_cl_rm %>%
  select(-riv_str_rc1, -hft_c9309_scaled_no_center) %>%
  pivot_longer(-c(siteid, cl),
    names_to = "response",
    values_to = "value") %>%
  group_by(cl, response) %>%
  summarise(value = median(value), .groups = "drop") %>%
  rename(cluster = cl)

# Ajouter moyenne par cluster
te <- te %>%
  left_join(avg_cl_var, by = c("cluster", "response"))

heat_map_df <- te %>%
  pivot_wider(names_from = "response", values_from = "value")
```

```{r}
cl_label <- c(
  "1: No change",
  "2: High turnover",
  "3: Increase of species richness",
  "4: Decrease of species richness",
  "5: Decrease of abundance",
  "6: Increase of abundance")
heat_cl <- te %>%
  ggplot(aes(
      x = get_var_replacement()[response],
      y = str_replace_all(as.character(cluster),
        setNames(cl_label, seq_len(6))),
      fill = value)) +
  geom_tile() +
  hrbrthemes::theme_ipsum(base_family = "Helvetica") +
  coord_fixed() +
  scale_fill_gradient2(
    high = muted("red"),
    mid = "white",
    low = muted("blue"),
    midpoint = 0,
    space = "Lab") +
  labs(x = "", y = "") +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))
```

```{r}
# Build heatmap
site_cl_rm$cl <- relevel(as.factor(site_cl_rm$cl), ref = 1)

cl_mod <- nnet::multinom(
  cl ~ riv_str_rc1 + hft_c9309_scaled_no_center,
  data = site_cl_rm)
```

```{r}
ci_cl_mod <- exp(confint(cl_mod)) - 1
exp_cl_mod <- exp(coef(cl_mod))
z_cl_mod <- summary(cl_mod)$coefficients / summary(cl_mod)$standard.errors


s_cl_mod <- map_dfr(list(exp_coef = exp_cl_mod, z = z_cl_mod),
  ~.x[, c("riv_str_rc1", "hft_c9309_scaled_no_center")] %>%
    as.data.frame() %>%
    rownames_to_column(var = "cluster"), .id = "type"
) %>%
  pivot_longer(c("riv_str_rc1", "hft_c9309_scaled_no_center"),
    names_to = "variable", values_to = "value") %>%
  pivot_wider(names_from = "type", values_from = "value") %>%
  mutate(pc = exp_coef - 1)
```

```{r}
p_cl_mod <- s_cl_mod %>%
  ggplot(aes(
      x = get_model_term_replacement()[variable],
      y = str_replace_all(as.character(cluster),
        setNames(cl_label, seq_len(6))),
      fill = pc
      )) +
  geom_tile(aes(alpha = abs(z))) +
  hrbrthemes::theme_ipsum(base_family = "Helvetica") +
  scale_fill_continuous(labels = scales::percent_format(accuracy = 1)) +
  coord_fixed() +
  labs(x = "", y = "", alpha = "Z-score", fill = "Probability") +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))
p_cl_mod
```

- Results of the multinomial model: 
  - 72% more probable to be in cluster 2 (high turnover) than in cluster 1 (no
    change) if PCA coordinates related to stream gradient increase by 1% 
  - 76% more probable to be in cluster 6 (increase in abundance) than in cluster
    1 if PCA coordinates related to stream gradient increase by 1% 
  - *I have to find a more clear story for this*
  - **We basically find the same results than the general model but more blurry**

# Test model with short-long term changes

- There is no multicollinearity among variables
- Human footprint 1993 =  lg term pressure / human footprint (1993-2009):
  short-term change 

```{r}
# Check the presence of multicollinearity without interaction
chao_mod_hft93 <- glmmTMB(log_chao_richness ~
   log1_year_nb + riv_str_rc1 +
     hft_ix_c9309_log2_ratio +
    hft_ix_c93 +
    (1 + log1_year_nb | main_bas/siteid),
  modelling_data
)
check_collinearity(chao_mod_hft93)
```

- Model with interaction between HFT 93 and river structure:

```{r}
chao_mod_hft93_int_riv <- glmmTMB(log_chao_richness ~
   log1_year_nb * riv_str_rc1 +
    log1_year_nb * hft_ix_c9309_log2_ratio +
    log1_year_nb * hft_ix_c93 +
    log1_year_nb * hft_ix_c93 * riv_str_rc1 +
    (1 + log1_year_nb | main_bas/siteid),
  modelling_data
)
diagnose(chao_mod_hft93_int_riv)
summary(chao_mod_hft93_int_riv)
```

```{r}
chao_mod_hft93_dbl_int_riv <- glmmTMB(log_chao_richness ~
   log1_year_nb * riv_str_rc1 +
    log1_year_nb * hft_ix_c9309_log2_ratio +
    log1_year_nb * hft_ix_c93 +
    log1_year_nb * hft_ix_c93 * riv_str_rc1 +
    log1_year_nb * hft_ix_c9309_log2_ratio * riv_str_rc1 +
    (1 + log1_year_nb | main_bas/siteid),
  modelling_data
)
diagnose(chao_mod_hft93_dbl_int_riv)
summary(chao_mod_hft93_dbl_int_riv)
```

- Below I test how much interaction the model can evaluate 
  - I show that the interaction among long-term, short-term changes of human
    footprint, stream gradient, and temporal trends (quadruple interaction)
    cannot be evaluated safely
  - However each triple interaction works!

```{r}
hb_mod <- glmmTMB(hillebrand_dis_scaled ~
  0 + log1_year_nb / riv_str_rc1 +
    log1_year_nb / hft_ix_c9309_log2_ratio +
    (0 + log1_year_nb | main_bas/siteid),
  modelling_data
)
summary(hb_mod)

hb_mod_hft93 <- glmmTMB(hillebrand_dis_scaled ~
  0 + log1_year_nb / riv_str_rc1 +
    log1_year_nb / hft_ix_c9309_log2_ratio +
    log1_year_nb / hft_ix_c93 +
    log1_year_nb / hft_ix_c93:hft_ix_c9309_log2_ratio +
    log1_year_nb / hft_ix_c93:riv_str_rc1 +
    (0 + log1_year_nb | main_bas/siteid),
  modelling_data
)
summary(hb_mod_hft93)
diagnose(hb_mod_hft93)

hb_mod_hft_full_int <- glmmTMB(hillebrand_dis_scaled ~
  0 + log1_year_nb / riv_str_rc1 +
    log1_year_nb / hft_ix_c9309_log2_ratio +
    log1_year_nb / hft_ix_c93 +
    log1_year_nb / hft_ix_c93:hft_ix_c9309_log2_ratio +
    log1_year_nb / hft_ix_c93:riv_str_rc1 +
    log1_year_nb / hft_ix_c9309_log2_ratio:riv_str_rc1 +
    log1_year_nb / hft_ix_c93:hft_ix_c9309_log2_ratio:riv_str_rc1 +
    (0 + log1_year_nb | main_bas/siteid),
  modelling_data
)
summary(hb_mod_hft_full_int)
diagnose(hb_mod_hft_full_int)
# Necessitate to delete the triple interaction?

chao_mod_hft_full_int <- glmmTMB(log_chao_richness ~
   log1_year_nb * riv_str_rc1 +
    log1_year_nb * hft_ix_c9309_log2_ratio +
    log1_year_nb * hft_ix_c93 +
    log1_year_nb:hft_ix_c93:hft_ix_c9309_log2_ratio +
    log1_year_nb:hft_ix_c93:riv_str_rc1 +
    log1_year_nb:hft_ix_c9309_log2_ratio:riv_str_rc1 +
    log1_year_nb:hft_ix_c93:hft_ix_c9309_log2_ratio:riv_str_rc1 +
    (1 + log1_year_nb | main_bas/siteid),
  modelling_data
)
diagnose(hb_mod_hft_full_int)
# Confirmed that we should remove the
summary(chao_mod_hft_full_int)
check_collinearity(chao_mod_hft_full_int)


hb_mod_hft_int_ok <- glmmTMB(hillebrand_dis_scaled ~
  0 + log1_year_nb / riv_str_rc1 +
    log1_year_nb / hft_ix_c9309_log2_ratio +
    log1_year_nb / hft_ix_c93 +
    log1_year_nb / hft_ix_c93:hft_ix_c9309_log2_ratio +
    log1_year_nb / hft_ix_c93:riv_str_rc1 +
    log1_year_nb / hft_ix_c9309_log2_ratio:riv_str_rc1 +
    (0 + log1_year_nb | main_bas/siteid),
  modelling_data
)
diagnose(hb_mod_hft_int_ok)
summary(hb_mod_hft_int_ok)

chao_mod_hft_int_ok <- glmmTMB(log_chao_richness ~
   log1_year_nb * riv_str_rc1 +
    log1_year_nb * hft_ix_c9309_log2_ratio +
    log1_year_nb * hft_ix_c93 +
    log1_year_nb:hft_ix_c93:hft_ix_c9309_log2_ratio +
    log1_year_nb:hft_ix_c93:riv_str_rc1 +
    log1_year_nb:hft_ix_c9309_log2_ratio:riv_str_rc1 +
    (1 + log1_year_nb | main_bas/siteid),
  modelling_data
)
diagnose(chao_mod_hft_int_ok)
# Confirmed that we should remove the
summary(chao_mod_hft_int_ok)
```

-

```{r}
hb_mod_hft_scaled_int_ok <- glmmTMB(hillebrand_dis_scaled ~
  0 + log1_year_nb / riv_str_rc1 +
    log1_year_nb / hft_c9309_scaled_no_center +
    log1_year_nb / hft_ix_c93 +
    log1_year_nb / hft_ix_c93:hft_c9309_scaled_no_center +
    log1_year_nb / hft_ix_c93:riv_str_rc1 +
    log1_year_nb / hft_c9309_scaled_no_center:riv_str_rc1 +
    (0 + log1_year_nb | main_bas/siteid),
  modelling_data
)
diagnose(hb_mod_hft_int_ok)
summary(hb_mod_hft_int_ok)
compare_parameters(hb_mod_hft_int_ok, hb_mod_hft_scaled_int_ok)
```


# Assess if log-year or year is the best model

Logging the year number is a way to linearize the relationship between
dissimilarity and year, which looks like a Michaelis-Menten relationship.
With the complete model, we can compare the fitting of the model with year or
log_year.

```{r}
#,
#   p_pred_obs = map(mod, plot_pred_obs_glmmtmb)
```


```{r}
biodiv_facets_main <- c(abun_var, var_jaccard, rich_var)[
  !str_detect(
    c(abun_var, var_jaccard, rich_var),
    "species_nb|tps|abundance_scaled|^total"
    )
  ]
```
- Model without random effect on ecological drivers, the more complex models having many
  convergence problems :

```{r}
comp_log_year_no_evt_re <- binded_gaussian_tmb_no_evt_re %>%
  filter(response %in% biodiv_facets_main) %>%
  mutate(
    fit_summary = map(mod, broom.mixed::glance)
  )
```

- Model year as log has a lower AIC except for total abundance and
  evenness_scaled. The difference of AIC is spectacular for jaccard and
  hillebrand.


```{r}
aic_table_no_evt_re <- comp_log_year_no_evt_re %>%
  select(year_var, response, fit_summary) %>%
  unnest(fit_summary) %>%
  select(year_var, response, AIC) %>%
  mutate( AIC = round(AIC)) %>%
  pivot_wider(names_from = "year_var", values_from = "AIC") %>%
  mutate(which_min = ifelse(
      log1_year_nb == min(c(year_nb, log1_year_nb)),
        "log1_year_nb", "year_nb")
  )
aic_table_no_evt_re
```

## Compare models 

- Actually, there are nothing to compare since the other models fails to have an
  AIC because the random effect are badly estimated.

```{r}
model_to_compare <- binded_gaussian_tmb_no_evt_re %>%
  filter(year_var == "log1_year_nb", response %in% biodiv_facets_main)
```

```{r}
model_to_compare <- model_to_compare %>%
  mutate(diagnosis = map_lgl(mod, diagnose))
```

- The site random effect on the intercept is producing problems: 

```{r}
x <- model_to_compare$mod[[2]] 
diagnose(x)
model_parameters(x)
```

```{r}
comp_jac <- tps_model_for_comp[tps_model_for_comp$response == "jaccard_dis_scaled",]$mod[[1]]

y <- update(x,
  formula = jaccard_dis_scaled ~ log1_year_nb * riv_str_rc1 + log1_year_nb *
    hft_ix_c9309_diff_scaled + (1 | main_bas/siteid) 
  )
diagnose(y)

#May be that the weak variation of log year nb is responsible 
z <- update(x,
  formula = jaccard_dis_scaled ~
    scale(log1_year_nb) * riv_str_rc1 +
    scale(log1_year_nb) * hft_ix_c9309_diff_scaled +
    (1 + scale(log1_year_nb) | main_bas/siteid)
  )
diagnose(z)
#no

za <- update(x,
  formula = jaccard_dis_scaled ~
    scale(log1_year_nb) * riv_str_rc1 +
    scale(log1_year_nb) * hft_ix_c9309_diff_scaled +
    (scale(log1_year_nb) | main_bas/siteid)
  )
diagnose(za)

zb <- update(x,
  formula = jaccard_dis_scaled ~
    log1_year_nb * riv_str_rc1 +
    log1_year_nb * hft_ix_c9309_diff_scaled +
    (1 | main_bas/siteid)
)
diagnose(zb)

zc <- comp_jac[["main0"]]

diagnose(zc)
parameters(zc)
confint(zc)
model_parameters(zc, bootstrap = FALSE)

yy <- parameters(y)
xx <- parameters(x)

```

- it looks like the the theta_1+log1_year_nb|site.3 makes a lot of trouble. I
  think that is a random effect but I do not know which one. Update: it is the
  random effect on intercept that seems to not work on temporal variables.
  It seems logical because intercept are all 0 


- Let's looks at the other problems

```{r}
walk2(model_to_compare$mod, model_to_compare$response,
  function(x, y) {
  print(y)
  diagnose(x)
  }
)
```

- Disappearance:

```{r}
comp_dis <- tps_model_for_comp[tps_model_for_comp$response == "disappearance_scaled",]$mod[[1]]
dis <- comp_dis[["main"]] 
dis2 <-  comp_dis[["main0"]]

model_parameters(dis2)

dis3 <- comp_dis[["no_main0"]]
```

It seems that I have unusually large statistics (Large Z) when the estimate  
is very low. I am not concerned about the estimations because the estimated
standard error on the estimates for large Z coefficients are of the same orders

```{r}
anova(dis3, dis2)
```

## Delete intercept for log year model ? 

The following plots displaying the coefficients of models with constrained intercept at 0 or not, main effect of environment or just the interaction with temporal trends, shows that the models for dissimilarity indices without main effects have the same interpretation. First the models with constrained intercept have a stronger coefficients of temporal trends. Second, in the models without main effect of environment, the interactions capture what was in the main effects before. 

- Test with jaccard model:

```{r}

zc0 <- comp_jac[["main0"]]

zc0_no_main <- comp_jac[["no_main0"]] 
```

```{r}
anova(zc0_no_main, zc0 ,zc)
```

```{r}
comp_intercept <- compare_models(
  tps_model_for_comp[
    tps_model_for_comp$response == "jaccard_dis_scaled", ]$mod[[1]], effects = "fixed")
comp_intercept %>%
  as_tibble() %>%
  select(Parameter, starts_with("CI_"), starts_with("Coefficient")) %>%
  pivot_longer(-Parameter, names_to = "names", values_to = "values") %>%
  separate(col = names, into  = c("type", "response"), sep = "\\.") %>%
  pivot_wider(names_from = "type", values_from = "values") %>%
  filter(!Parameter %in% "(Intercept)", !str_detect(Parameter, "unitabundance")) %>%
  filter(!response %in% c("species_nb", "log_species_nb", "total_abundance", "total_abundance_scaled")) %>%
  ggplot(
    aes(y = Parameter, x = Coefficient, color = response, xmin = CI_low,
      xmax = CI_high)) +
  geom_pointrange(position = position_dodge(width = .2))

```

- Nestedness:

```{r}
comp_nestedness <- tps_model_for_comp[tps_model_for_comp$response == "nestedness_scaled", ]$mod[[1]]
anova(comp_nestedness[["main"]], comp_nestedness[["main0"]], comp_nestedness[["no_main0"]])
```

```{r}
model_comp_interp_int <- function(resp =NULL, df = modelling_data) {

  ne <- glmmTMB(formula = as.formula(paste0(resp, "~
        log1_year_nb * riv_str_rc1 +
        log1_year_nb * hft_ix_c9309_diff_scaled +
        (0 + log1_year_nb | main_bas/siteid)")),
  data = df 
  )

  ne0 <- update(ne,
    formula = as.formula(paste0(resp, "~
        0 + log1_year_nb * riv_str_rc1 +
        log1_year_nb * hft_ix_c9309_diff_scaled +
        (0 + log1_year_nb | main_bas/siteid)"))
  )

  ne0_no_main <- update(ne,  
    formula = as.formula(paste0(resp, "~
        0 + log1_year_nb / riv_str_rc1 +
        log1_year_nb / hft_ix_c9309_diff_scaled +
        (0 + log1_year_nb | main_bas/siteid)"))
  )

  list(main = ne, main0 = ne0, no_main0 = ne0_no_main)
}
```



```{r}
comp_ne <- compare_models(tps_model_for_comp[tps_model_for_comp$response == "nestedness_scaled", ]$mod[[1]], effects = "fixed")

#HERE display plot
```

- Turnover:


```{r}
tu_comp <- tps_model_for_comp[tps_model_for_comp$response == "turnover_scaled", ]$mod[[1]]
anova(tu_comp[[1]], tu_comp[[2]], tu_comp[[3]])
aic <- map_dfr(tu_comp, broom.mixed::glance, .id = "name")
round(exp((min(aic$AIC) - aic$AIC)/2), 3)
```


```{r}
plot_model_comp_coeff(model_list = tu_comp)
```

- Appearance:

```{r}
appearance_comp <- model_comp_interp_int(resp = "appearance_scaled")
anova(appearance_comp[[1]], appearance_comp[[2]], appearance_comp[[3]])
```

```{r}
plot_model_comp_coeff(model_list = appearance_comp)
```


## Variable distribution

###

```{r}
skim(modelling_data,
  "jaccard_dis", "turnover", "nestedness", "appearance",
  "disappearance", "hillebrand", "log_chao_richness",
  "log_total_abundance")
```



###

## Analysis

### Rigal method result


#### Trends according to Rigal method 

```{r}
ti <- map_dfr(rigal_trends, ~tabyl_df(x = .x, group = "direction"),
  .id = "response"
)
ti %>%
  filter(direction != "Total", response %in% clust_var) %>%
  select(response, direction, percent) %>%
  mutate(response = str_replace_all(response, get_var_replacement())) %>%
  pivot_wider(names_from = "direction", values_from = "percent") %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "hover"))
```

```{r}
ti <- map_dfr(rigal_trends, ~tabyl_df(x = .x, group = "shape_class"),
  .id = "response"
)

ti %>%
  filter(shape_class != "Total") %>%
  select(response, shape_class, percent) %>%
  mutate(response = str_replace_all(response, get_var_replacement())) %>%
  pivot_wider(names_from = "shape_class", values_from = "percent") %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "hover"))
```

#### Comparison of our results with previous studies (rigal method)

```{r}
tar_load(rigal_slp_df)
```



```{r}
knitr::opts_chunk$set(eval = FALSE)
```


```{r, fig.height=12}
ti <- expand.grid(
  resp1 = unique(slope$response),
  resp2 = unique(slope$response)
  ) %>%
  filter(resp2 != resp1) %>%
  filter(
    resp1 %in% c("chao_richness", "species_nb", "log_species_nb",
      "total_abundance", "log_total_abundance")) %>%
  mutate_all(as.character) %>%
  arrange(resp1)

test <- map2(ti$resp1, ti$resp2,
  function(x, y) {
    bi <- slope %>%
      filter(response %in% c(x, y)) %>%
      select(siteid, response, linear_slope) %>%
      pivot_wider(names_from = "response", values_from = "linear_slope")

    return(bi)
  }
)

p_trends_trends <- map(test, function(x) {
  l <- colnames(x)
  x %>%
    ggplot(aes(x = !!sym(l[2]), y = !!sym(l[3]))) +
    geom_point() +
    geom_smooth(method = "loess") +
    labs(
      x = get_var_replacement()[l[2]],
      y = get_var_replacement()[l[3]]
    )
})
names(p_trends_trends) <- map_chr(test, ~colnames(.x)[2])
```

```{r, fig.height=12}
plot_grid(
  plotlist = p_trends_trends[names(p_trends_trends) %in% "log_species_nb"],
  ncol = 3
)
```

#### Maps

```{r, eval=FALSE}
rigal_trends_df_loc <- filtered_dataset$location %>%
  left_join(rigal_trends_df, by = "siteid") %>%
  st_as_sf(coords = c("longitude", "latitude"),
  crs = 4326)
```

## Reproducibility

<details><summary>Reproducibility receipt</summary>

```{r}
## datetime
Sys.time()

## repository
if(requireNamespace('git2r', quietly = TRUE)) {
  git2r::repository()
} else {
  c(
    system2("git", args = c("log", "--name-status", "-1"), stdout = TRUE),
    system2("git", args = c("remote", "-v"), stdout = TRUE)
  )
}

## session info
sessionInfo()
```

</details>
